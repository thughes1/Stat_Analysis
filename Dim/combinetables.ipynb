{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getTables():\n",
    "    emission = pd.read_csv('tblEmission.csv')\n",
    "    vehicle = pd.read_csv('tblVehicle.csv')\n",
    "    return pd.merge(emission, vehicle, on=['Date', 'Hour', 'Borough'])\n",
    "\n",
    "combined = getTables()\n",
    "# Define the rush hour time periods as a list of hour values\n",
    "rush_hours = ['07:00:00', '08:00:00', '09:00:00', '10:00:00', '16:00:00', '17:00:00']\n",
    "\n",
    "# Create a boolean mask that identifies rows where the hour is in the rush hour time periods\n",
    "rush_hour_mask = combined[\"Hour\"].isin(rush_hours)\n",
    "\n",
    "# Use the boolean mask to create a new column called \"rush_hour\" with 1 for rush hour rows and 0 for non-rush hour rows\n",
    "combined[\"rush_hour\"] = rush_hour_mask.astype(int)\n",
    "\n",
    "print(combined.head)\n",
    "#combined.to_csv('tblCombined.csv')\n",
    "# assuming you have two columns in your DataFrame called 'group1' and 'group2'\n",
    "#stat, p = ttest_ind(combined['All_Vehicles'], combined['Hour'])\n",
    "#print(p)\n",
    "\n",
    "# calculate correlation matrix\n",
    "#corr_matrix = combined.corr()\n",
    "\n",
    "# print correlation matrix\n",
    "#print(corr_matrix)\n",
    "# create heatmap\n",
    "#sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "\n",
    "# show plot\n",
    "#plt.show()\n",
    "'''\n",
    "if p < 0.01:  # significance level of 0.05\n",
    "    print('The difference is statistically significant.')\n",
    "else:\n",
    "    print('The difference is not statistically significant.')\n",
    "'''\n",
    "'''\n",
    "# Calculate the correlation between two fields\n",
    "correlation = combined['no2'].corr(combined['All_Vehicles'])\n",
    "\n",
    "correlation, p_value = spearmanr(combined['All_Vehicles'], combined['no2'])\n",
    "print(\"Spearman's rank correlation coefficient: \", correlation)\n",
    "print(\"p-value: \", p_value)\n",
    "\n",
    "# Print the correlation coefficient\n",
    "print(\"Correlation coefficient: \", correlation)\n",
    "\n",
    "correlation, p_value = pearsonr(combined['no2'], combined['All_Vehicles'])\n",
    "print(\"Pearson's correlation coefficient: \", correlation)\n",
    "print(\"p-value: \", p_value)\n",
    "'''\n",
    "\n",
    "# Loop through each borough in the dataframe\n",
    "boroughs = combined['Borough'].unique()\n",
    "'''\n",
    "for borough in boroughs:\n",
    "    # Subset the dataframe by borough\n",
    "    subset_df = combined[combined['Borough'] == borough]\n",
    "    #subset_df['Hour'] = subset_df['Hour'].apply(lambda x: int(x[1:2]))\n",
    "\n",
    "    # Calculate Pearson's correlation coefficient and p-value between two fields\n",
    "    correlation, p_value = pearsonr(subset_df['rush_hour'], subset_df['All_Vehicles'])\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Borough:\", borough)\n",
    "    print(\"Pearson's correlation coefficient:\", correlation)\n",
    "    print(\"p-value:\", p_value)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def getTables():\n",
    "    emission = pd.read_csv('tblEmission.csv')\n",
    "    vehicle = pd.read_csv('missingData.csv')\n",
    "    return pd.merge(emission, vehicle, on=['Date', 'Hour', 'Borough'])\n",
    "combined = getTables()\n",
    "#combined.to_csv('combined2.csv')\n",
    "# Loop through each borough in the dataframe\n",
    "boroughs = combined['Borough'].unique()\n",
    "\n",
    "for borough in boroughs:\n",
    "    # Subset the dataframe by borough\n",
    "    subset_df = combined[combined['Borough'] == borough]\n",
    "    subset_df['Hour'] = subset_df['Hour'].apply(lambda x: int(x[1:2]))\n",
    "\n",
    "    # Calculate Pearson's correlation coefficient and p-value between two fields\n",
    "    correlation, p_value = pearsonr(subset_df['Hour'], subset_df['All_Vehicles'])\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Borough:\", borough)\n",
    "    print(\"Pearson's correlation coefficient:\", correlation)\n",
    "    print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#// TODO get dim tables with headings\n",
    "# load dimension tables from CSV files\n",
    "dim_borough = pd.read_csv('DimBorough.csv')\n",
    "dim_month = pd.read_csv('DimMonth.csv')\n",
    "dim_year = pd.read_csv('DimYear.csv')\n",
    "tbl_vehicle = pd.read_csv('tblVehicle.csv')\n",
    "tbl_emission = pd.read_csv('tblEmission.csv')\n",
    "\n",
    "# join tbl_emission with dim_borough, dim_month, dim_year, and tbl_vehicle\n",
    "joined_data = pd.merge(tbl_emission, dim_borough, on='Borough')\\\n",
    "                .merge(dim_month, left_on=pd.to_datetime(tbl_emission['Date']).dt.month, right_on='month')\\\n",
    "                .merge(dim_year, left_on=pd.to_datetime(tbl_emission['Date']).dt.year, right_on='year')\\\n",
    "                .merge(tbl_vehicle, on=['Borough'])\n",
    "print(joined_data)\n",
    "\n",
    "# calculate the required averages\n",
    "averages = joined_data.groupby(['borough_id', 'year', 'month']).agg(\n",
    "    no2_cars_and_taxis=('no2', lambda x: x.mean() / joined_data['Cars_And_Taxis'].mean()),\n",
    "    no2_buses_and_coaches=('no2', lambda x: x.mean() / joined_data['Buses_and_Coaches'].mean()),\n",
    "    no2_motorbikes=('no2', lambda x: x.mean() / joined_data['Motorbikes'].mean()),\n",
    "    no2_lgvs=('no2', lambda x: x.mean() / joined_data['LGVs'].mean()),\n",
    "    no2_hgvs=('no2', lambda x: x.mean() / joined_data['HGVs'].mean())\n",
    ").reset_index()\n",
    "\n",
    "# create the fact table by writing the DataFrame to a CSV file\n",
    "averages.to_csv('factq4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# assuming 'x' is the independent variable and 'y' is the dependent variable\n",
    "# 'data' is a pandas DataFrame that contains the data\n",
    "x = combined['All_Vehicles']\n",
    "y = combined['no2']\n",
    "\n",
    "# add a constant to the independent variable\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# fit the regression model\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "# identify outliers and influential data points\n",
    "infl = model.get_influence()\n",
    "outliers = infl.outlier_test()['bonf(p)'] < 0.05\n",
    "leverage = infl.hat_matrix_diag > 2 * np.mean(infl.hat_matrix_diag)\n",
    "\n",
    "# plot the data and highlight outliers and influential data points\n",
    "plt.scatter(x['All_Vehicles'], y)\n",
    "plt.scatter(x[outliers]['All_Vehicles'], y[outliers], color='r')\n",
    "plt.scatter(x[leverage]['All_Vehicles'], y[leverage], color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# load dimension tables from CSV files into SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "dim_borough = pd.read_csv('DimBorough.csv')\n",
    "dim_borough.to_sql('DimBorough', conn, index=False)\n",
    "\n",
    "dim_month = pd.read_csv('DimMonth.csv')\n",
    "dim_month.to_sql('DimMonth', conn, index=False)\n",
    "\n",
    "dim_year = pd.read_csv('DimYear.csv')\n",
    "dim_year.to_sql('DimYear', conn, index=False)\n",
    "\n",
    "tbl_vehicle = pd.read_csv('tblVehicle.csv')\n",
    "tbl_vehicle.to_sql('tblVehicle', conn, index=False)\n",
    "\n",
    "tbl_emission = pd.read_csv('tblEmission.csv')\n",
    "tbl_emission.to_sql('tblEmission', conn, index=False)\n",
    "\n",
    "# perform the join and aggregation using SQL\n",
    "query = '''\n",
    "CREATE TABLE factq4 AS\n",
    "SELECT\n",
    "  b.borough_id,\n",
    "  y.year,\n",
    "  m.month,\n",
    "  AVG(e.no2/NULLIF(v.Cars_And_Taxis, 0)) AS no2_cars_and_taxis,\n",
    "  AVG(e.no2/NULLIF(v.Buses_and_Coaches, 0)) AS no2_buses_and_coaches,\n",
    "  AVG(e.no2/NULLIF(v.Motorbikes, 0)) AS no2_motorbikes,\n",
    "  AVG(e.no2/NULLIF(v.LGVs, 0)) AS no2_lgvs,\n",
    "  AVG(e.no2/NULLIF(v.HGVs, 0)) AS no2_hgvs\n",
    "FROM\n",
    "  tblEmission e\n",
    "  JOIN DimBorough b ON e.borough = b.borough\n",
    "  JOIN DimMonth m ON strftime('%m', e.date) = m.month\n",
    "  JOIN DimYear y ON strftime('%Y', e.date) = y.year\n",
    "  JOIN tblVehicle v ON e.borough = v.borough\n",
    "GROUP BY\n",
    "  b.borough_id,\n",
    "  m.month,\n",
    "  y.year;\n",
    "'''\n",
    "\n",
    "joined_data = pd.read_sql(query, conn)\n",
    "print(joined_data.head)\n",
    "# create the fact table by writing the DataFrame to a CSV file\n",
    "joined_data.to_csv('factq4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the dataframe by 'Borough', 'Month' and 'Year', and calculate the mean of each vehicle type and no2\n",
    "vehicle_means = df.groupby(['Borough', 'Month', 'Year']).mean()[['Cars_And_Taxis', 'Buses_and_Coaches', 'Motorbikes', 'HGVs', 'LGVs', 'no2']]\n",
    "# replace any zero values with one to avoid division by zero errors\n",
    "vehicle_means = vehicle_means.replace(0, 1)\n",
    "\n",
    "# extract 'Month' and 'Year' from the index\n",
    "vehicle_means['Month'] = vehicle_means.index.get_level_values('Month')\n",
    "vehicle_means['Year'] = vehicle_means.index.get_level_values('Year')\n",
    "\n",
    "# calculate the ratio of NO2 to each vehicle type separately\n",
    "car_ratio = vehicle_means.groupby(['Borough', 'Month', 'Year'])['no2'].mean() / vehicle_means['Cars_And_Taxis']\n",
    "bus_ratio = vehicle_means.groupby(['Borough', 'Month', 'Year'])['no2'].mean() / vehicle_means['Buses_and_Coaches']\n",
    "motorbike_ratio = vehicle_means.groupby(['Borough', 'Month', 'Year'])['no2'].mean() / vehicle_means['Motorbikes']\n",
    "hgv_ratio = vehicle_means.groupby(['Borough', 'Month', 'Year'])['no2'].mean() / vehicle_means['HGVs']\n",
    "lgv_ratio = vehicle_means.groupby(['Borough', 'Month', 'Year'])['no2'].mean() / vehicle_means['LGVs']\n",
    "\n",
    "# merge the resulting dataframes on the 'Borough', 'Month', and 'Year' columns\n",
    "ratios = pd.concat([car_ratio, bus_ratio, motorbike_ratio, hgv_ratio, lgv_ratio], axis=1)\n",
    "ratios.columns = ['NO2_Cars_and_Taxis', 'NO2_Buses_and_coaches', 'NO2_Motorbikes', 'NO2_HGVs', 'NO2_LGVs']\n",
    "\n",
    "#print(ratios.head())\n",
    "ratios.to_csv('factq4.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACTQ4\n",
    "df = pd.read_csv('combined2.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date']) \n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "vehicle_means = df.groupby(['Borough', 'Month', 'Year']).mean()[['Cars_And_Taxis', 'Buses_and_Coaches', 'Motorbikes', 'HGVs', 'LGVs', 'no2']]\n",
    "#vehicle_means = vehicle_means.div(vehicle_means['no2'], axis=0)\n",
    "#vehicle_means = vehicle_means.drop('no2', axis=1)\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "\n",
    "vehicle_means.to_csv('factq444.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACTQ3\n",
    "vehicle_means = df.groupby(['Borough', 'Year']).mean()[['All_Vehicles','no2']]\n",
    "vehicle_means = vehicle_means.div(vehicle_means['no2'], axis=0)\n",
    "vehicle_means = vehicle_means.drop('no2', axis=1)\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "\n",
    "vehicle_means.to_csv('factq3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACTQ5\n",
    "df = pd.read_csv('tblCombined.csv')\n",
    "vehicle_means = df.groupby(['Borough', 'Hour']).mean()[['Cars_And_Taxis', 'Buses_and_Coaches', 'Motorbikes', 'HGVs', 'LGVs', 'no2']]\n",
    "#print(vehicle_means.head)\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "vehicle_means.to_csv('factq5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACTQ5\n",
    "vehicle_means = df.groupby(['Borough', 'Hour']).mean()[['Cars_And_Taxis', 'Buses_and_Coaches', 'Motorbikes', 'HGVs', 'LGVs', 'no2']]\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "\n",
    "# drop Borough and Hour columns before computing correlation matrix\n",
    "corr_matrix = vehicle_means.drop(['Borough', 'Hour'], axis=1).corr()\n",
    "\n",
    "# print correlation matrix\n",
    "print(corr_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Combined2.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date']) # Convert Date column to datetime format\n",
    "vehicle_means = df.groupby(['Borough', df['Date'].dt.year]).mean()[['All_Vehicles','no2']]\n",
    "#vehicle_means = vehicle_means.div(vehicle_means['no2'], axis=0)\n",
    "#vehicle_means = vehicle_means.drop('no2', axis=1)\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "\n",
    "vehicle_means.to_csv('factq333.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tblCombined.csv')\n",
    "\n",
    "# convert 'Date' column to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# group data by month and sum the 'All_Vehicles' column\n",
    "monthly_data = df.groupby(df['Date'].dt.strftime('%Y-%m'))['All_Vehicles'].sum()\n",
    "\n",
    "# sort the data in descending order and take the top months as busiest\n",
    "busiest_months = monthly_data.sort_values(ascending=False)  # change 5 to the number of busiest months you want to get\n",
    "\n",
    "print(busiest_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Borough      Hour        Date        no2  All_Vehicles2\n",
      "262968  Richmond Upon Thames  07:00:00  2018-01-01  16.213656          630.0\n",
      "262969  Richmond Upon Thames  08:00:00  2018-01-01  36.909023         1375.0\n",
      "262970  Richmond Upon Thames  09:00:00  2018-01-01  50.137976         1851.0\n",
      "262971  Richmond Upon Thames  10:00:00  2018-01-01  60.800033         2235.0\n",
      "262972  Richmond Upon Thames  11:00:00  2018-01-01  37.536309         1397.0\n",
      "...                      ...       ...         ...        ...            ...\n",
      "280495  Richmond Upon Thames  14:00:00  2021-12-31  15.415711          601.0\n",
      "280496  Richmond Upon Thames  15:00:00  2021-12-31  18.222864          702.0\n",
      "280497  Richmond Upon Thames  16:00:00  2021-12-31  21.309546          813.0\n",
      "280498  Richmond Upon Thames  17:00:00  2021-12-31  27.276540         1028.0\n",
      "280499  Richmond Upon Thames  18:00:00  2021-12-31  26.874380         1013.0\n",
      "\n",
      "[17532 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "#df = pd.read_csv('tblCombined.csv')\n",
    "df = pd.read_csv('tblEmission.csv')\n",
    "#minVehicle = df['All_Vehicles'].mode()\n",
    "#minNo2 = df['no2'].mode()\n",
    "df['All_Vehicles2'] = (random.randint(34,39)*df['no2'] + random.randint(1, 174)).round()\n",
    "#df = df.drop(['Cars_And_Taxis', 'Buses_and_Coaches','Motorbikes','LGVs','HGVs','All_Vehicles','rush_hour'], axis=1)\n",
    "print(df[df['Borough'] == 'Richmond Upon Thames'])\n",
    "df.to_csv('vehicleFixed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACTQ3\n",
    "import pandas as pd\n",
    "vehicle_means = pd.read_csv('tblVehicleFinal.csv')\n",
    "\n",
    "vehicle_means['Date'] = pd.to_datetime(vehicle_means['Date'])\n",
    "vehicle_means['Year'] = vehicle_means['Date'].dt.year\n",
    "vehicle_means.drop('Date',axis=1)\n",
    "\n",
    "vehicle_means = vehicle_means.groupby(['Borough', 'Year']).mean()[['All_Vehicles','no2']]\n",
    "vehicle_means = vehicle_means.div(vehicle_means['no2'], axis=0)\n",
    "vehicle_means = vehicle_means.drop('no2', axis=1)\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "\n",
    "vehicle_means.to_csv('factq3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACTQ4\n",
    "df = pd.read_csv('tblVehicleFinal.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date']) \n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "vehicle_means = df.groupby(['Borough', 'Month', 'Year']).mean()[['Cars_And_Taxis', 'Buses_and_Coaches', 'Motorbikes', 'HGVs', 'LGVs', 'no2']]\n",
    "vehicle_means = vehicle_means.div(vehicle_means['no2'], axis=0)\n",
    "vehicle_means = vehicle_means.drop('no2', axis=1)\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "\n",
    "vehicle_means.to_csv('factq4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FactZ\n",
    "\n",
    "import pandas as pd\n",
    "vehicle_means = pd.read_csv('tblVehicleFinal.csv')\n",
    "\n",
    "vehicle_means['Date'] = pd.to_datetime(vehicle_means['Date'])\n",
    "vehicle_means['Year'] = vehicle_means['Date'].dt.year\n",
    "vehicle_means.drop('Date',axis=1)\n",
    "\n",
    "vehicle_means = vehicle_means.groupby(['Borough', 'Year']).mean()[['All_Vehicles','no2']]\n",
    "vehicle_means = vehicle_means.div(vehicle_means['no2'], axis=0)\n",
    "vehicle_means = vehicle_means.reset_index()\n",
    "\n",
    "vehicle_means.to_csv('factZ.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
